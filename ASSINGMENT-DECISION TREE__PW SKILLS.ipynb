{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354fb192-c944-40c4-b66e-066cd30a1395",
   "metadata": {},
   "source": [
    "#### 1. What is a Decision Tree, and how does it work in the context of \n",
    "classification?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c03fd64-415b-47bc-95e7-ec235110c8e2",
   "metadata": {},
   "source": [
    "A decision tree is a supervised machine learning algorithm that works like a flowchart to classify data into distinct categories by creating a tree-like structure of decisions. It starts with a root node and branches out, with each internal node representing a test on a feature and each leaf node representing the final classification or outcome. In classification, the tree splits data recursively based on feature values to create pure, homogeneous subsets at the leaf nodes, enabling it to predict the class of new data points by traversing down the branches.\n",
    " \n",
    "How a Decision Tree Works in Classification\n",
    "1. Root Node:\n",
    "The process begins at the root node, which is the initial question or test based on a specific feature of the dataset. \n",
    "\n",
    "2. Branches:\n",
    "The branches represent the possible outcomes or answers (e.g., \"yes\" or \"no,\" or a value range) to the test at an internal node. \n",
    "\n",
    "3. Internal Nodes:\n",
    "Each branch leads to an internal node, which poses another question or performs another test on a different feature. \n",
    "\n",
    "4. Recursive Splitting:\n",
    "This splitting process continues, recursively dividing the data into smaller and smaller subsets based on the feature tests. The goal is to create subsets where all instances belong to the same class, a state known as a \"pure\" node. \n",
    "\n",
    "5. Leaf Nodes:\n",
    "When a split results in a pure subset, it becomes a leaf node, representing a final predicted class label for that data. \n",
    "\n",
    "6. Prediction:\n",
    "To classify a new data instance, you start at the root node and follow the branches based on the instance's feature values until you reach a leaf node, which provides the predicted class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dae80a-b1c8-41b3-b7bb-c94cf1b450ae",
   "metadata": {},
   "source": [
    "#### 2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\r\n",
    "How do they impact the splits in a Decision Tree?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c4d26c3-6bdc-497b-a1b9-3127f801e660",
   "metadata": {},
   "source": [
    "Gini Impurity and Entropy are both metrics for measuring the \"impurity\" or disorder of data within a node in a decision tree. Gini Impurity quantifies the probability of an incorrect classification if a data point is randomly assigned to a class, while Entropy measures the uncertainty or disorder of the node's data distribution. In decision trees, the algorithm selects splits that minimize the impurity (maximize purity) of the child nodes, aiming to create more homogeneous, pure leaf nodes with the lowest possible Gini or Entropy values. \n",
    "\n",
    "Gini Impurity Explained:-\n",
    "\n",
    "Concept: Gini Impurity measures how often a randomly selected element from a dataset would be incorrectly classified if it were randomly labeled based on the class distribution at that node. \n",
    "\n",
    "Range: It ranges from 0 to 1. \n",
    "\n",
    "Interpretation:\n",
    "0: Represents a perfectly \"pure\" node, meaning all data points belong to the same class. \n",
    "1: Represents a maximum impurity, where data points are randomly distributed across different classes. \n",
    "\n",
    "Entropy Explained:-\n",
    "\n",
    "Concept: Entropy measures the amount of uncertainty or disorder in a set of data. It quantifies the expected \"information,\" \"surprise,\" or \"uncertainty\" associated with the potential outcomes of a random variable. \n",
    "\n",
    "Range: It also ranges from 0 to 1. \n",
    "\n",
    "Interpretation:\n",
    "0: Denotes a pure node with no uncertainty or disorder. \n",
    "1: Denotes the most impure node, such as a 50-50 split between two classes, indicating maximum uncertainty. \n",
    "\n",
    "Impact on Splits in a Decision Tree:-\n",
    "\n",
    "Both Gini Impurity and Entropy serve as splitting criteria in decision trees to determine the best feature and split point. \n",
    "1. Measuring Node Purity:\n",
    "At each node, the algorithm calculates the impurity (either Gini or Entropy) for the current node. \n",
    "\n",
    "2. Evaluating Splits:\n",
    "It then evaluates all possible splits for each feature and their corresponding thresholds. \n",
    "\n",
    "3. Calculating Child Impurity:\n",
    "For each potential split, the impurity is calculated for the resulting child nodes. \n",
    "\n",
    "4. Weighted Average:\n",
    "A weighted average impurity for the child nodes is computed, based on the number of instances in each child node. \n",
    "\n",
    "5. Selecting the Best Split:\n",
    "The split that results in the lowest weighted average impurity is selected as the best split for that node. \n",
    "\n",
    "6. Goal:\n",
    "The ultimate goal is to reduce impurity from the root node down to the leaf nodes, creating a tree that effectively separates data into pure, distinct categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecaff58-3e91-4caa-87f6-b9dba3ec4aa6",
   "metadata": {},
   "source": [
    "#### 3. What is the difference between Pre-Pruning and Post-Pruning in Decision\r\n",
    "Trees? Give one practical advantage of using each.\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a2395b0-45ca-468d-9393-8f226408f4b4",
   "metadata": {},
   "source": [
    "Pre-pruning stops a decision tree's growth during training by setting conditions to prevent it from becoming too complex, while post-pruning builds a full tree and then removes unnecessary branches to simplify it and improve generalization. A practical advantage of pre-pruning is its computational efficiency because it avoids building unnecessary parts of the tree. A practical advantage of post-pruning is its robustness, as it often produces a more accurate model than pre-pruning because it doesn't suffer from a greedy approach that might prematurely stop growth.\n",
    " \n",
    "Pre-Pruning (Early Stopping):-\n",
    "\n",
    "How it works:\n",
    "During the construction of the decision tree, growth is stopped based on specific criteria, such as a maximum depth or a minimum number of samples in a node. \n",
    "\n",
    "Practical Advantage:\n",
    "Computational efficiency and Speed. By not growing the tree to its full potential, it saves computation time and resources, making it more efficient for large datasets. \n",
    "\n",
    "Post-Pruning (Backward Pruning):-\n",
    "\n",
    "How it works:\n",
    "A complete decision tree is first generated, and then branches that do not significantly contribute to the model's performance are removed or simplified. \n",
    "\n",
    "Practical Advantage:\n",
    "Improved Accuracy and Robustness. By pruning after the tree is built, it can be more effective at identifying and removing irrelevant branches, often leading to a more accurate and robust final model than pre-pruning, which can be too aggressive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f392c-daa0-4403-bf45-fa32e09ecc1d",
   "metadata": {},
   "source": [
    "#### 4.What is Information Gain in Decision Trees, and why is it important for\r\n",
    "choosing the best split?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3933feed-76ef-4fb2-84da-0be6b3f659e1",
   "metadata": {},
   "source": [
    "Information Gain measures the reduction in entropy (data impurity or randomness) after a dataset is split using a particular feature, indicating how much that feature helps to classify the data. It's important for choosing the best split because decision tree algorithms select the feature that yields the highest Information Gain, as this feature best separates the data into more homogeneous (purer) subsets, leading to a more effective and accurate classification tree. \n",
    "\n",
    "What is Information Gain?\n",
    "\n",
    "Measures Reduction in Uncertainty:\n",
    "Information Gain quantifies how much the uncertainty or \"impurity\" in a dataset decreases after it's split by a specific attribute. \n",
    "\n",
    "Based on Entropy:\n",
    "It is calculated by subtracting the weighted average entropy of the child nodes from the entropy of the parent node. \n",
    "\n",
    "A Proxy for Purity:\n",
    "A higher Information Gain indicates a more effective split, resulting in subsets that are more \"pure\" (contain instances of a single class). \n",
    "\n",
    "Why is it Important for Choosing the Best Split?\n",
    "\n",
    "Identifies the Most Informative Feature:\n",
    "At each internal node of a decision tree, the algorithm evaluates the Information Gain for every available feature. \n",
    "\n",
    "Maximizes Purity:\n",
    "The feature with the maximum Information Gain is chosen for the split because it creates subsets that are the most distinct and well-separated. \n",
    "\n",
    "Builds a More Effective Tree:\n",
    "By maximizing Information Gain, the decision tree construction process ensures that the most useful features are prioritized, leading to a more homogeneous and pure classification at the leaf nodes. \n",
    "\n",
    "Greedy Approach:\n",
    "The algorithm is \"greedy,\" meaning it selects the best split at each step without looking ahead or backtracking, making Information Gain the critical metric for making these local decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3356e3-e36c-487a-a0bb-3edbcef0e190",
   "metadata": {},
   "source": [
    "#### 5. What are some common real-world applications of Decision Trees, and\r\n",
    "what are their main advantages and limitations?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d6613e6-9e26-487b-a6f9-096e0be871cf",
   "metadata": {},
   "source": [
    "Decision trees are used in healthcare for diagnoses, in finance for credit risk assessment, and in marketing for customer segmentation. Their main advantages include interpretability, ease of use, and the ability to handle both numerical and categorical data, while limitations are overfitting, instability with data changes, a bias towards features with many levels, and potential for poor generalization on new data. \n",
    "\n",
    "Common Real-World Applications:-\n",
    "Healthcare:\n",
    "To assist in diagnosing diseases by analyzing patient symptoms and medical history. \n",
    "\n",
    "Finance:\n",
    "For credit scoring, risk assessment, and fraud detection in financial transactions. \n",
    "\n",
    "Marketing:\n",
    "To understand customer behavior, predict customer churn, and for targeted marketing strategies. \n",
    "\n",
    "Retail:\n",
    "In inventory management by predicting sales trends and optimizing stock levels.\n",
    "\n",
    "Manufacturing:\n",
    "For process optimization by analyzing factors that impact operational efficiency. \n",
    "\n",
    "Telecommunications:\n",
    "To predict and identify customer churn based on usage patterns. \n",
    "\n",
    "Advantages:-\n",
    "\n",
    "Understandable & Interpretable:\n",
    "Decision trees are easy to understand, with logical choices and tangible results, making complex decisions clear. \n",
    "\n",
    "Handles Diverse Data:\n",
    "They can process both numerical and categorical data, making them versatile for various datasets. \n",
    "\n",
    "Simple Data Preparation:\n",
    "Requires relatively little data preparation compared to other complex machine learning algorithms. \n",
    "\n",
    "Handles Non-linear Relationships:\n",
    "Decision trees can capture and model complex, non-linear relationships within the data. \n",
    "\n",
    "Limitations:-\n",
    "Overfitting:\n",
    "Trees can become too complex and deep, leading to poor performance on new, unseen data because they perfectly fit the training data. \n",
    "\n",
    "Instability:\n",
    "Small variations or noise in the data can result in completely different tree structures, making them sensitive to data changes. \n",
    "\n",
    "Bias Towards Dominant Classes:\n",
    "In datasets with imbalanced class distributions, the decision tree may favor the dominant class, leading to biased predictions. \n",
    "\n",
    "Limited Precision for Complex Problems:\n",
    "The tree structure can struggle to capture certain complex relationships effectively, potentially limiting precision for highly intricate problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502291bf-f52a-4221-84ab-b18bb07d5b04",
   "metadata": {},
   "source": [
    "#### 6. Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ad4b60-a407-4cdc-9b23-70797e3991ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.00\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display feature importances\n",
    "importances = clf.feature_importances_\n",
    "print(\"Feature Importances:\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502191f-bce5-4aeb-839a-c87e10cbbe2a",
   "metadata": {},
   "source": [
    "#### 7. Write a Python program to:\r\n",
    "● Load the Iris Dataset\r\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\r\n",
    "a fully-grown tree.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2d766d-e214-4052-bd82-1436717b9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with max_depth=3: 1.00\n",
      "Accuracy with fully-grown tree: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model 1: Decision Tree with max_depth=3\n",
    "tree_depth3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "tree_depth3.fit(X_train, y_train)\n",
    "y_pred_depth3 = tree_depth3.predict(X_test)\n",
    "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
    "\n",
    "# Model 2: Fully-grown Decision Tree (no max_depth)\n",
    "tree_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "y_pred_full = tree_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy with max_depth=3: {accuracy_depth3:.2f}\")\n",
    "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb2e05-c24c-4be4-903b-d6a36f87ec80",
   "metadata": {},
   "source": [
    "#### 8. Write a Python program to:\r\n",
    "● Load the California Housing dataset from sklearn\r\n",
    "● Train a Decision Tree Regressor\r\n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df73cd1-18b5-47be-87ea-debb83ae5215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4952\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5285\n",
      "HouseAge: 0.0519\n",
      "AveRooms: 0.0530\n",
      "AveBedrms: 0.0287\n",
      "Population: 0.0305\n",
      "AveOccup: 0.1308\n",
      "Latitude: 0.0937\n",
      "Longitude: 0.0829\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Display feature importances\n",
    "importances = regressor.feature_importances_\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411feeb-7054-409b-98c3-4041d6588d7c",
   "metadata": {},
   "source": [
    "#### 9.  Write a Python program to:\r\n",
    "● Load the Iris Dataset\r\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\r\n",
    "GridSearchCV\r\n",
    "● Print the best parameters and the resulting model accuracy\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba141df8-efc5-4543-941c-1906a41f54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "# Create Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for best parameters\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5ba18-3e61-4d23-aae7-f20869fccff9",
   "metadata": {},
   "source": [
    "#### 10. Imagine you’re working as a data scientist for a healthcare company that\r\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\r\n",
    "mixed data types and some missing values.\r\n",
    "Explain the step-by-step process you would follow to:\r\n",
    "● Handle the missing values\r\n",
    "● Encode the categorical features\r\n",
    "● Train a Decision Tree model\r\n",
    "● Tune its hyperparameters\r\n",
    "● Evaluate its performance\r\n",
    "And describe what business value this model could provide in the real-world\r\n",
    "setting.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a987d96-4ae6-4faf-97f7-df6a1e0e6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ---------------------\n",
    "# 1. Load your dataset\n",
    "# ---------------------\n",
    "# Replace this with your actual data loading logic\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # placeholder\n",
    "target_col = 'disease_present'        # binary target column (0 or 1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=target_col)\n",
    "y = df[target_col]\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Define preprocessing steps\n",
    "# ------------------------------\n",
    "# Impute numerical and categorical features\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "cat_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_imputer, numerical_cols),\n",
    "    ('cat', Pipeline(steps=[\n",
    "        ('imputer', cat_imputer),\n",
    "        ('encoder', cat_encoder)\n",
    "    ]), categorical_cols)\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Build pipeline with model\n",
    "# -----------------------------\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Split data and define grid search\n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 5, 10, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Evaluate best model\n",
    "# -----------------------------\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "634ef560-1d2d-4db4-bd80-c5c612056e39",
   "metadata": {},
   "source": [
    "Business Value in a Real-World Setting\n",
    "Implementing this model can bring tangible business and societal value:\n",
    "\n",
    "1. Early Disease Detection\n",
    "Helps identify at-risk patients sooner, leading to timely intervention.\n",
    "\n",
    "Reduces progression and complications.\n",
    "\n",
    "2. Improved Clinical Decisions\n",
    "Acts as a decision-support tool for doctors, highlighting high-risk patients.\n",
    "\n",
    "Enables personalized treatment plans.\n",
    "\n",
    "3. Cost Reduction\n",
    "Preventive care is often cheaper than treating late-stage disease.\n",
    "\n",
    "Reduces unnecessary tests by targeting the right patients.\n",
    "\n",
    "4. Resource Optimization\n",
    "Helps hospitals allocate resources (staff, diagnostics) more efficiently.\n",
    "\n",
    "5. Compliance and Risk Management\n",
    "Aids in meeting quality benchmarks (e.g., avoid missed diagnoses).\n",
    "\n",
    "Strengthens reputation with regulators and insurers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4616341-25fd-4e57-b783-3677e9d27c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
