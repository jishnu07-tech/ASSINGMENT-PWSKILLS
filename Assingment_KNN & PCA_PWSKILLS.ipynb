{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c826ca-f893-4f8a-bccb-d9be94ac17bb",
   "metadata": {},
   "source": [
    "#### 1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
    "classification and regression problems?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da969681-6f61-4d24-8add-81e2cf6cf0dd",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple, non-parametric, supervised learning algorithm used for both classification and regression tasks by identifying the 'k' closest training data points to a new, unclassified point. For classification, it assigns the new point to the majority class of its 'k' nearest neighbors. For regression, it predicts a continuous value by taking the average or mean of the target values of its 'k' nearest neighbors. \n",
    "\n",
    "How KNN Works (The Three Main Steps)\n",
    "1. Calculate Distances:\n",
    "A distance metric (like Euclidean distance) is used to calculate the distance between the new data point (the query point) and every other data point in the training dataset. \n",
    "\n",
    "2. Select K Nearest Neighbors:\n",
    "The algorithm identifies the 'k' data points that are closest to the query point based on the calculated distances. \n",
    "\n",
    "3. Make a Prediction:\n",
    "For Classification: The new data point is assigned the class that is most frequent among its 'k' nearest neighbors. \n",
    "\n",
    "For Regression: A continuous value is predicted by calculating the average or mean of the target values (the actual numbers) of the 'k' nearest neighbors. \n",
    "\n",
    "Example\n",
    "Imagine you have data points representing different fruits, categorized by color and shape. \n",
    "\n",
    "For Classification:\n",
    "If you introduce a new fruit, and its 3 (k=3) closest neighbors in the data are 2 apples and 1 banana, the new fruit would be classified as an apple because it has the majority class label among its neighbors. \n",
    "\n",
    "For Regression:\n",
    "If you were predicting the price of a house based on its location and size, and the 5 (k=5) nearest neighbors to the new house had prices of 200k, 220k, 250k, 210k, and 230k, the price of the new house would be estimated as the average of these values (e.g., 222k).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52336489-63eb-43c1-b794-78f548702ebe",
   "metadata": {},
   "source": [
    "#### 2. What is the Curse of Dimensionality and how does it affect KNN\n",
    "performance?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37eb1ddf-5859-4433-a6b9-a6e843fa192a",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality describes how a dataset's complexity and sparsity increase exponentially with more features, making it difficult for algorithms to find patterns and leading to performance degradation, a phenomenon particularly detrimental to the K-Nearest Neighbors (KNN) algorithm. KNN's reliance on distance metrics becomes unreliable in high dimensions, as all points tend to become equidistant from each other, making it challenging to identify truly \"nearest\" neighbors and resulting in less accurate predictions.  \n",
    "\n",
    "What is the Curse of Dimensionality?\n",
    "\n",
    "Increased Data Sparsity:\n",
    "As the number of features (dimensions) grows, the feature space expands exponentially, making data points very sparse and far apart. \n",
    "\n",
    "Need for More Data:\n",
    "To achieve the same level of accuracy and density as in lower-dimensional spaces, an exponentially larger amount of data is required.\n",
    "\n",
    "Loss of Meaningful Distances:\n",
    "In high-dimensional spaces, the differences between distances become less significant. Data points become nearly the same distance from each other, reducing the ability to distinguish truly \"close\" neighbors from \"distant\" ones. \n",
    "\n",
    "Increased Computational Cost:\n",
    "More dimensions mean more calculations for algorithms, leading to higher computational complexity. \n",
    "\n",
    "How it Affects KNN Performance:-\n",
    "\n",
    "Decreased Accuracy:\n",
    "Because distances become less discriminative, KNN struggles to find relevant neighbors, leading to less accurate predictions and classifications. \n",
    "\n",
    "Overfitting:\n",
    "With so many features, KNN can easily overfit the training data by focusing on noisy features rather than true underlying patterns. \n",
    "\n",
    "Challenges with Distance Metrics:\n",
    "KNN's fundamental assumption that nearby points are similar breaks down in high dimensions. The concept of \"nearby\" loses its meaning. \n",
    "\n",
    "Higher Data Requirements:\n",
    "As dimensions increase, KNN requires exponentially more training data to maintain performance, which is often impractical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994431e7-fddf-4426-8e97-80c9cacb8e89",
   "metadata": {},
   "source": [
    "#### 3. What is Principal Component Analysis (PCA)? How is it different from\n",
    "feature selection?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6032c358-90a8-40b7-ab8b-6d5634fd68ec",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a feature extraction technique that transforms original features into new, uncorrelated variables called principal components, which capture the most variance in the data, thereby reducing dimensionality. Feature selection, in contrast, is a process of selecting a subset of the original features based on their predictive power or relevance to the target variable, thus retaining interpretability. The key difference is that PCA creates new, derived features, while feature selection keeps the original features.  \n",
    "\n",
    "Principal Component Analysis (PCA)\n",
    "What it is:\n",
    "A dimensionality reduction technique that transforms original features into a smaller set of new features called principal components. \n",
    "\n",
    "How it works:\n",
    "It finds linear combinations of the original features that are orthogonal (perpendicular) and explain the maximum possible variance in the data. \n",
    "\n",
    "Purpose:\n",
    "To reduce data redundancy, simplify models, and make large datasets more manageable for machine learning algorithms. \n",
    "\n",
    "Pros:\n",
    "Reduces the number of variables, helps identify underlying data structure, and can improve model performance. \n",
    "\n",
    "Cons:\n",
    "The resulting principal components are often difficult to interpret because they are linear combinations of the original features. \n",
    "\n",
    "Feature Selection\n",
    "What it is:\n",
    "A method for choosing a subset of the most relevant and informative original features from the dataset. \n",
    "\n",
    "How it works:\n",
    "It evaluates the importance of each original feature based on its relationship with the target variable and then selects the features that are most useful. \n",
    "\n",
    "Purpose:\n",
    "To remove irrelevant or redundant features, reduce noise, and improve model interpretability and accuracy. \n",
    "\n",
    "Pros:\n",
    "Maintains the interpretability of the original features, making it easier to understand the model's workings. \n",
    "\n",
    "Cons:\n",
    "Does not create new features, and the selection process can be computationally intensive. \n",
    "\n",
    "Key Differences\n",
    "Feature Transformation vs. Selection:\n",
    "PCA is a feature extraction method, meaning it creates entirely new features (principal components) from the original ones. Feature selection, on the other hand, keeps the original features but discards some of them. \n",
    "\n",
    "Interpretability:\n",
    "Feature selection methods generally result in more interpretable models because they retain the original, human-readable features. PCA often sacrifices interpretability for data reduction and variance explanation. \n",
    "\n",
    "Goal:\n",
    "PCA aims to capture the most important information by creating new, uncorrelated variables. Feature selection aims to identify and keep the most predictive original features for a specific task, such as classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cf013-2de4-41cf-b4b0-bc97a0990ca9",
   "metadata": {},
   "source": [
    "#### 4. What are eigenvalues and eigenvectors in PCA, and why are they\r\n",
    "important?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f220ca9-e2d1-4a61-9a82-c40bcb685305",
   "metadata": {},
   "source": [
    "In PCA, eigenvectors define the directions of maximum variance in the data (the new axes, or principal components), while eigenvalues indicate the magnitude of variance along those directions. Eigenvectors are important because they provide the principal components that capture the most significant patterns in the data, and eigenvalues are crucial for selecting these components by quantifying how much information (variance) each component retains. \n",
    "\n",
    "What are Eigenvectors in PCA?\n",
    "\n",
    "Directions of Variance:\n",
    "Eigenvectors represent the principal components, which are the new axes in a transformed coordinate system that capture the directions of greatest variability in the dataset. \n",
    "\n",
    "Undistorted Vectors:\n",
    "When a matrix (in PCA, the covariance matrix) is multiplied by an eigenvector, the eigenvector's direction remains unchanged; it is only scaled by a factor. \n",
    "\n",
    "Key for New Basis:\n",
    "The set of eigenvectors forms a new basis for the data, which is orthogonal to the original coordinate system. \n",
    "\n",
    "What are Eigenvalues in PCA?\n",
    "Magnitude of Variance:\n",
    "The corresponding eigenvalue for each eigenvector indicates the amount of variance present in the direction of that eigenvector. \n",
    "\n",
    "Information Content:\n",
    "A larger eigenvalue signifies a direction that explains more variance in the data, while a smaller eigenvalue indicates a direction with less variance.\n",
    "\n",
    "Proportion of Total Variance:\n",
    "Eigenvalues can be expressed as a percentage of the total variance in the dataset, showing how much information each principal component captures. \n",
    "\n",
    "Why are they Important in PCA?\n",
    "Dimensionality Reduction:\n",
    "By identifying the eigenvectors with the largest eigenvalues, PCA can select the most informative dimensions and discard those with minimal variance, thus reducing the complexity of the data. \n",
    "\n",
    "Pattern Identification:\n",
    "Eigenvectors highlight the underlying linear patterns and structure in the data by pointing to the directions where the data varies the most. \n",
    "\n",
    "Data Transformation:\n",
    "Eigenvalues and eigenvectors facilitate the transformation of the original data into a new, lower-dimensional space where the data is more interpretable and where principal components are uncorrelated. \n",
    "\n",
    "Feature Selection:\n",
    "They provide a quantitative way to determine which principal components are most important for further analysis, aiding in feature selection and building more efficient models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f77497-6c8b-49f3-847c-6b7bb6f2f3bc",
   "metadata": {},
   "source": [
    "#### 5. How do KNN and PCA complement each other when applied in a single\r\n",
    "pipeline?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6adcfb85-9298-44dd-af81-896d00b5f32a",
   "metadata": {},
   "source": [
    "PCA complements KNN in a pipeline by acting as a dimensionality reduction technique that reduces noise, multicollinearity, and computational complexity for the KNN algorithm, especially with high-dimensional data. This enhances KNN's performance by projecting the data into a lower-dimensional space while preserving essential information, making distance calculations more efficient and less susceptible to the \"curse of dimensionality\".  \n",
    "\n",
    "How PCA Benefits KNN: -\n",
    "\n",
    "Reduces Dimensionality:\n",
    "KNN's performance can suffer with high-dimensional data, where the presence of many features can decrease accuracy and increase computational cost. PCA reduces the number of features to a smaller set of principal components, simplifying the distance calculations for KNN.\n",
    "\n",
    "Minimizes Noise and Redundancy:\n",
    "PCA can effectively remove noise and multicollinearity (highly correlated features) in the dataset, leading to a more stable and efficient model for KNN.\n",
    "\n",
    "Improves Computational Efficiency:\n",
    "By transforming data into a lower-dimensional space, PCA significantly reduces the number of features used in the KNN algorithm's distance calculations, which speeds up the process.\n",
    "\n",
    "Addresses the Curse of Dimensionality:\n",
    "High-dimensional data can violate the assumptions of distance-based algorithms like KNN, leading to poor performance. PCA helps to mitigate this effect by creating a more informative and compact representation of the data.\n",
    "\n",
    "Enhances Model Stability:\n",
    "Eliminating redundant information and noise makes the resulting features more robust, which in turn leads to a more stable and accurate KNN classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6698f15-374e-40b9-8588-4c392a20a84c",
   "metadata": {},
   "source": [
    "#### 6. Train a KNN Classifier on the Wine dataset with and without feature\r\n",
    "scaling. Compare model accuracy in both cases.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3c6914-4d61-4a3e-b29d-8621347c5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy without Scaling: 0.7222222222222222\n",
      "KNN Accuracy with Scaling   : 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN without Scaling\n",
    "# -------------------------\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_no_scaling = knn.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# -------------------------\n",
    "# 2. KNN with Scaling\n",
    "# -------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# -------------------------\n",
    "# Results\n",
    "# -------------------------\n",
    "print(\"KNN Accuracy without Scaling:\", acc_no_scaling)\n",
    "print(\"KNN Accuracy with Scaling   :\", acc_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfb062-8c64-44d4-a6ae-24174e297458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1650e02b-53af-4532-aa9a-90acc1de594d",
   "metadata": {},
   "source": [
    "#### 7. Train a PCA model on the Wine dataset and print the explained variance\r\n",
    "ratio of each principal component.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc830d4-46ac-4fa8-aefc-34f34f66ead8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of Each Principal Component:\n",
      "PC1: 0.3620\n",
      "PC2: 0.1921\n",
      "PC3: 0.1112\n",
      "PC4: 0.0707\n",
      "PC5: 0.0656\n",
      "PC6: 0.0494\n",
      "PC7: 0.0424\n",
      "PC8: 0.0268\n",
      "PC9: 0.0222\n",
      "PC10: 0.0193\n",
      "PC11: 0.0174\n",
      "PC12: 0.0130\n",
      "PC13: 0.0080\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Print explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
    "for i, ratio in enumerate(explained_variance, start=1):\n",
    "    print(f\"PC{i}: {ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb572aaa-4a39-41d8-854a-c0293354cdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e1b9271-e409-424a-bde3-149a0eb46b15",
   "metadata": {},
   "source": [
    "#### 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
    "components). Compare the accuracy with the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc7cac7-62c2-40ea-8f87-8dd2ff4c3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy on Original Scaled Data: 0.9444444444444444\n",
      "KNN Accuracy on PCA (2 components):   0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN on Original Scaled Data\n",
    "# -------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = knn_original.predict(X_test_scaled)\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# -------------------------\n",
    "# 2. KNN on PCA-Transformed Data (Top 2 Components)\n",
    "# -------------------------\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# -------------------------\n",
    "# Results\n",
    "# -------------------------\n",
    "print(\"KNN Accuracy on Original Scaled Data:\", acc_original)\n",
    "print(\"KNN Accuracy on PCA (2 components):  \", acc_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1114db-03d1-4add-ac26-892e8ebd32fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e923a0e-3a54-433c-9abd-d907cdfc4c87",
   "metadata": {},
   "source": [
    "#### 9. Train a KNN Classifier with different distance metrics (euclidean,\r\n",
    "manhattan) on the scaled Wine dataset and compare the results.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8143c6c4-aa9d-467c-93ec-a4a645c144d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (Euclidean): 0.9444444444444444\n",
      "KNN Accuracy (Manhattan): 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN with Euclidean Distance\n",
    "# -------------------------\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
    "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# -------------------------\n",
    "# 2. KNN with Manhattan Distance\n",
    "# -------------------------\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# -------------------------\n",
    "# Results\n",
    "# -------------------------\n",
    "print(\"KNN Accuracy (Euclidean):\", acc_euclidean)\n",
    "print(\"KNN Accuracy (Manhattan):\", acc_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15788be9-cb43-466b-b15a-d07a344abbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46a0007-6cd5-4142-9533-0d11dc9edb29",
   "metadata": {},
   "source": [
    " #### You are working with a high-dimensional gene expression dataset to\n",
    "classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models\n",
    "overfit.\n",
    "Explain how you would:\n",
    "● Use PCA to reduce dimensionality\n",
    "● Decide how many components to keep\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "● Evaluate the model\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
    "biomedical data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c543e90e-77d0-47ba-88c4-24c623cd6c23",
   "metadata": {},
   "source": [
    "🔹 1. Use PCA to Reduce Dimensionality\n",
    "\n",
    "Why PCA? Gene expression datasets often have tens of thousands of features (genes), many of which are correlated. PCA projects this high-dimensional data onto a lower-dimensional space by creating principal components (PCs) that capture the maximum variance.\n",
    "\n",
    "How it helps: By reducing dimensionality, we remove noise and redundancy, making the model more generalizable and computationally efficient.\n",
    "\n",
    "🔹 2. Decide How Many Components to Keep\n",
    "\n",
    "Explained Variance Ratio: Look at the cumulative explained variance plot (scree plot). A common choice is to retain components that explain ~90–95% of the variance.\n",
    "\n",
    "Cross-Validation Approach: Instead of arbitrarily picking, we can evaluate classification accuracy with different numbers of components using cross-validation, and choose the smallest number of PCs that yields strong performance.\n",
    "\n",
    "Domain Consideration: In biomedical research, interpretability matters. Sometimes fewer PCs (that separate cancer subtypes well) are better, even if they capture less variance.\n",
    "\n",
    "🔹 3. Use KNN for Classification Post-Dimensionality Reduction\n",
    "\n",
    "After PCA, the dataset is transformed into fewer, uncorrelated features.\n",
    "\n",
    "KNN is then applied in this reduced space.\n",
    "\n",
    "Why KNN? It’s non-parametric, simple, and makes no assumptions about data distribution (important for biological data, which is rarely normally distributed).\n",
    "\n",
    "Distances are more meaningful in reduced space — avoiding the “curse of dimensionality.”\n",
    "\n",
    "🔹 4. Evaluate the Model\n",
    "\n",
    "Cross-Validation (e.g., stratified k-fold): Since the dataset is small, k-fold cross-validation ensures robust performance estimates.\n",
    "\n",
    "Metrics: Accuracy is useful, but in biomedical contexts we’d also check precision, recall, F1-score, and confusion matrices to ensure good performance across all cancer types.\n",
    "\n",
    "External Validation: If possible, test on an independent dataset (another patient cohort) to prove generalizability.\n",
    "\n",
    "🔹 5. Justify the Pipeline to Stakeholders\n",
    "\n",
    "Robustness: PCA prevents overfitting by removing noise and redundancy from high-dimensional gene data.\n",
    "\n",
    "Efficiency: Reducing thousands of genes to a few dozen components makes computation feasible and speeds up predictions.\n",
    "\n",
    "Interpretability: While PCA itself produces abstract components, we can often trace back loadings to identify key genes or pathways driving classification, which is valuable for biomedical insight.\n",
    "\n",
    "Generalizability: Using cross-validation and PCA ensures the model is not memorizing patients, but truly learning patterns that generalize to unseen data.\n",
    "\n",
    "Real-World Relevance: Biomedical datasets are almost always small-sample, high-dimensional. This approach is standard in bioinformatics and has proven successful in cancer subtype classification studies.\n",
    "\n",
    " In short:\n",
    "We reduce complexity with PCA, carefully select the number of components, use KNN in the reduced space to classify patients, and rigorously evaluate via cross-validation. This balances accuracy, robustness, and interpretability, making it a sound solution for real-world cancer prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edaecb3-2e80-4506-aa69-7568904722ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
