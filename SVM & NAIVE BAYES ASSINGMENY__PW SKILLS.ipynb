{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513ea30e-9ec2-4e61-b587-39bd22fe961f",
   "metadata": {},
   "source": [
    "#### 1. What is a Support Vector Machine (SVM), and how does it work?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6428ec5-bc7a-40c6-8e15-ef56b43a0122",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding the optimal hyperplane that separates data points into different classes, maximizing the margin between them. This margin is defined by \"support vectors,\" which are the data points closest to the hyperplane. SVMs can handle both linear and non-linear data by mapping it into a higher-dimensional space using kernel functions. \n",
    "\n",
    "Here's a breakdown of how SVMs work:\n",
    "\n",
    "1. Finding the Optimal Hyperplane: \n",
    "In a multi-dimensional space, a hyperplane acts as a decision boundary. \n",
    "SVM aims to find the hyperplane that best separates the different classes of data points with the widest possible margin. \n",
    "The margin is the distance between the hyperplane and the closest data points (support vectors). \n",
    "Maximizing the margin helps in better generalization of the model, meaning it performs well on unseen data. \n",
    "\n",
    "2. Support Vectors:\n",
    "These are the data points that lie closest to the hyperplane and influence its position. \n",
    "SVM focuses on these critical points to define the decision boundary, ignoring other data points. \n",
    "\n",
    "3. Handling Non-linear Data: \n",
    "For data that is not linearly separable (cannot be separated by a straight line or hyperplane), SVM uses kernel functions to map the data into a higher-dimensional space where it becomes linearly separable. \n",
    "Common kernel functions include polynomial kernels, radial basis function (RBF) kernels, and sigmoid kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fe62a-910e-4821-9c14-7ee48f4b9611",
   "metadata": {},
   "source": [
    "#### 2. Explain the difference between Hard Margin and Soft Margin SVM.\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53510692-60d1-4b44-9e84-0fada0f8f116",
   "metadata": {},
   "source": [
    "Hard margin and soft margin SVMs are two variations of the Support Vector Machine algorithm used for classification. Hard margin SVMs require perfectly separable data and aim to find a hyperplane that maximizes the margin between classes with no misclassifications. Soft margin SVMs, on the other hand, are designed for datasets that are not perfectly separable, allowing for some misclassifications (or \"errors\") to achieve a larger margin and better generalization. \n",
    "\n",
    "Hard Margin SVM:\n",
    "\n",
    "Assumption: Data is linearly separable (can be perfectly divided by a hyperplane). \n",
    "Goal: Find the hyperplane that maximizes the margin while ensuring all data points are on the correct side of the margin. \n",
    "Constraint: No misclassifications are allowed. \n",
    "Sensitivity to outliers: Highly sensitive to outliers, as a single outlier can significantly impact the position of the separating hyperplane. \n",
    "Example: If there are a few points that are very close to the decision boundary, the hard margin SVM might create a very narrow margin to classify them correctly. \n",
    "\n",
    "Soft Margin SVM:\n",
    "\n",
    "Assumption: Data may not be perfectly linearly separable or may contain outliers. \n",
    "Goal: Maximize the margin while allowing for a certain number of misclassifications. \n",
    "Constraint: Introduces \"slack variables\" that allow for some data points to be on the wrong side of the margin or even on the wrong side of the hyperplane. \n",
    "Flexibility: More flexible and robust to noisy or non-linearly separable data. \n",
    "Regularization parameter: Uses a regularization parameter (C) to control the trade-off between maximizing the margin and minimizing the number of misclassifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e407b-59f6-457f-aeb7-455828aab27a",
   "metadata": {},
   "source": [
    "#### 3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35aed5fa-ab17-4765-8b9f-364fc52ce989",
   "metadata": {},
   "source": [
    "The Kernel Trick is a technique used in Support Vector Machines (SVMs) and other kernel methods to handle non-linearly separable data. When data cannot be separated by a straight line or hyperplane in its original input space, the Kernel Trick implicitly maps the data into a higher-dimensional feature space where it may become linearly separable. The \"trick\" lies in the fact that this mapping is performed without explicitly calculating the coordinates in the higher-dimensional space, instead relying on kernel functions that compute the dot product between the transformed data points. This significantly reduces computational complexity.\n",
    "\n",
    "One example of a kernel is the Radial Basis Function (RBF) Kernel, also known as the Gaussian Kernel.\n",
    "\n",
    "RBF Kernel Use Case:\n",
    "The RBF Kernel is widely used when the relationship between data points is non-linear and complex. It is particularly effective when dealing with data that forms concentric circles or other non-linear clusters, where a linear separator would fail.\n",
    "\n",
    "Code:\n",
    "K(x_i, x_j) = exp(-γ * ||x_i - x_j||^2)\n",
    "\n",
    "Here, x_i and x_j are two data points, ||x_i - x_j||^2 is the squared Euclidean distance between them, and γ (gamma) is a hyperparameter that controls the influence of individual training samples. A larger γ means closer points have a stronger influence, leading to a more complex decision boundary, while a smaller γ results in a smoother boundary.\n",
    "For instance, in image classification, if you have images of different objects that are not linearly separable based on pixel values alone, the RBF kernel can help transform these images into a higher-dimensional space where a linear decision boundary can effectively separate the different object classes. It's also frequently used in bioinformatics for gene expression analysis or in financial modeling for predicting stock prices, where complex non-linear relationships exist within the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469d1a1-2f7f-4a73-b580-5f391a381965",
   "metadata": {},
   "source": [
    "#### 4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bea84d1b-ffea-4bc9-89c1-d14661ecba17",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a simple probabilistic classifier that applies Bayes' theorem with strong independence assumptions between the features. It's called \"naive\" because it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. This simplifying assumption, while often unrealistic, makes the algorithm computationally efficient and surprisingly effective in many real-world scenarios, particularly text classification. \n",
    "\n",
    "Elaboration:\n",
    "\n",
    "Bayes' Theorem:\n",
    "The core of the classifier is Bayes' theorem, which provides a way to calculate the probability of a hypothesis (in this case, a class) given some observed evidence (the features).\n",
    "\n",
    "Independence Assumption:\n",
    "The \"naive\" part comes from the assumption that all features are independent of each other given the class. In other words, the presence or absence of one feature does not affect the probability of another feature being present, given the class. \n",
    "\n",
    "Why \"Naive\"?:\n",
    "This independence assumption is often not true in real-world data. For example, in text classification, the presence of the word \"discount\" is likely related to the presence of the word \"sale\". However, despite this simplification, Naive Bayes classifiers can still perform well, especially in tasks like spam filtering, document categorization, and sentiment analysis. \n",
    "\n",
    "Advantages:\n",
    "\n",
    "Despite its simplicity, Naive Bayes classifiers have several advantages:\n",
    "Efficiency: They are computationally inexpensive and fast to train, especially on large datasets. \n",
    "Scalability: They scale well with the number of features. \n",
    "Effectiveness: They can be surprisingly accurate, particularly when the independence assumption holds even approximately. \n",
    "Ease of Implementation: They are relatively easy to implement and understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdf8a5-a832-4381-9ce4-3d42d08d9e2c",
   "metadata": {},
   "source": [
    "#### 5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. \n",
    "When would you use each one?\r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f26bf1f0-7f68-40c8-812b-6f92206914df",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers come in several variants, each suited for different types of data. Gaussian Naive Bayes is used for continuous data that follows a normal distribution, Multinomial Naive Bayes is best for discrete data like word counts in text, and Bernoulli Naive Bayes is suitable for binary data where features represent presence or absence. \n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "\n",
    "Data Type:\n",
    "Continuous, typically assumed to be normally distributed. \n",
    "Example:\n",
    "Classifying flower species based on petal length and width, where these measurements are assumed to be continuous and follow a normal distribution. \n",
    "How it works:\n",
    "Estimates the mean and variance of each feature for each class and uses the Gaussian probability density function to calculate probabilities. \n",
    "When to use:\n",
    "When dealing with features that can take on continuous values and are approximately normally distributed, such as measurements of physical characteristics. \n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "Data Type:\n",
    "Discrete, often representing counts or frequencies. \n",
    "Example:\n",
    "Classifying emails as spam or not spam based on word frequencies in the email text. \n",
    "How it works:\n",
    "Assumes features follow a multinomial distribution, where each feature value represents the number of times a specific outcome is observed. \n",
    "When to use:\n",
    "When working with data where features represent counts or frequencies, such as word counts in text documents or item counts in a purchase history. \n",
    "\n",
    "3. Bernoulli Naive Bayes:\n",
    "Data Type:\n",
    "Binary or Boolean, indicating the presence or absence of a feature. \n",
    "Example:\n",
    "Classifying documents based on whether specific words are present or absent (e.g., \"spam\" or \"not spam\" emails). \n",
    "How it works:\n",
    "Assumes features are binary, with values like 0 or 1, and uses the Bernoulli distribution to model feature probabilities. \n",
    "When to use:\n",
    "When features are binary or represent the presence or absence of something, such as the presence of a word in a text document or a user clicking on an advertisement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c43d68-0289-42bc-bb11-031ff42a338b",
   "metadata": {},
   "source": [
    "#### 6. Write a Python program to:\r\n",
    "● Load the Iris dataset\r\n",
    "● Train an SVM Classifier with a linear kernel\r\n",
    "● Print the model's accuracy and support vectors.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25e0d50-f5d0-43c8-96a2-090b4bbba960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "Support Vectors:\n",
      " [[4.8 3.4 1.9 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.9 2.5 4.5 1.7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train an SVM classifier with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Print model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Print support vectors\n",
    "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351277a-8d3a-4aeb-8420-3cd7bfa8dc55",
   "metadata": {},
   "source": [
    "#### 7. Write a Python program to:\r\n",
    "● Load the Breast Cancer dataset\r\n",
    "● Train a Gaussian Naïve Bayes model\r\n",
    "● Print its classification report including precision, recall, and F1-score.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d700d50-6ae6-4b1a-b360-1058fe7b9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.90      0.92        63\n",
      "      benign       0.95      0.96      0.95       108\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gaussian Naïve Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bbcc7-292a-4e30-9736-57b3b1ca5f42",
   "metadata": {},
   "source": [
    "#### 8.  Write a Python program to:\r\n",
    "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\r\n",
    "C and gamma.\r\n",
    "● Print the best hyperparameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794a7eb6-bab0-464a-848a-5f7c328eab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Test Accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for C and gamma\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# GridSearchCV with 5-fold cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print best hyperparameters and accuracy\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970a90f-36ee-4707-9b1d-622a2b79047a",
   "metadata": {},
   "source": [
    "#### 9. Write a Python program to:\r\n",
    "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\r\n",
    "sklearn.datasets.fetch_20newsgroups).\r\n",
    "● Print the model's ROC-AUC score for its predictions.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4328f775-5adf-4437-bb73-74401be102c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9999773689094078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "\r\n",
    "# Load a subset of 20 Newsgroups (binary classification for ROC-AUC)\r\n",
    "categories = ['sci.space', 'rec.sport.baseball']\r\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\r\n",
    "\r\n",
    "X = newsgroups.data\r\n",
    "y = newsgroups.target  # Binary labels\r\n",
    "\r\n",
    "# Convert text to TF-IDF features\r\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\r\n",
    "X_tfidf = vectorizer.fit_transform(X)\r\n",
    "\r\n",
    "# Split into train and test\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    X_tfidf, y, test_size=0.3, random_state=42\r\n",
    ")\r\n",
    "\r\n",
    "# Train Naïve Bayes classifier\r\n",
    "nb_model = MultinomialNB()\r\n",
    "nb_model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Predict probabilities\r\n",
    "y_prob = nb_model.predict_proba(X_test)[:, 1]\r\n",
    "\r\n",
    "# Compute ROC-AUC score\r\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\r\n",
    "print(\"ROC-AUC Score:\", roc_auc)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05d39d-43de-406b-a8a0-2dd20c7f6dfd",
   "metadata": {},
   "source": [
    "#### 10. Imagine you’re working as a data scientist for a company that handles\r\n",
    "email communications.\r\n",
    "Your task is to automatically classify emails as Spam or Not Spam. The emails may\r\n",
    "contain:\r\n",
    "● Text with diverse vocabulary\r\n",
    "● Potential class imbalance (far more legitimate emails than spam)\r\n",
    "● Some incomplete or missing data\r\n",
    "Explain the approach you would take to:\r\n",
    "● Preprocess the data (e.g. text vectorization, handling missing data)\r\n",
    "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\r\n",
    "● Address class imbalance\r\n",
    "● Evaluate the performance of your solution with suitable metrics\r\n",
    "And explain the business impact of your solution.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777e8b65-3bf8-4f3b-9f75-70b310858f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multinomial Naïve Bayes ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.92      0.98      0.95       299\n",
      "         sci.space       0.97      0.92      0.95       296\n",
      "\n",
      "          accuracy                           0.95       595\n",
      "         macro avg       0.95      0.95      0.95       595\n",
      "      weighted avg       0.95      0.95      0.95       595\n",
      "\n",
      "Confusion Matrix:\n",
      " [[292   7]\n",
      " [ 24 272]]\n",
      "\n",
      "=== Linear SVM (with class weights) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.91      0.97      0.94       299\n",
      "         sci.space       0.97      0.91      0.94       296\n",
      "\n",
      "          accuracy                           0.94       595\n",
      "         macro avg       0.94      0.94      0.94       595\n",
      "      weighted avg       0.94      0.94      0.94       595\n",
      "\n",
      "Confusion Matrix:\n",
      " [[291   8]\n",
      " [ 28 268]]\n",
      "\n",
      "ROC-AUC (Naïve Bayes): 0.9939437765524721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load dataset (simulating spam vs. not spam)\n",
    "categories = ['rec.sport.baseball', 'sci.space']  # \"spam\" vs \"ham\"\n",
    "data = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = data.data\n",
    "y = data.target  # 0/1 labels\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Define TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# 3A. Naïve Bayes pipeline\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# 3B. Linear SVM pipeline with class weighting\n",
    "# Compute class weights for imbalance\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('clf', LinearSVC(class_weight=class_weights))\n",
    "])\n",
    "\n",
    "# 4. Train and evaluate Naïve Bayes\n",
    "print(\"=== Multinomial Naïve Bayes ===\")\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "y_pred_nb = nb_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_nb, target_names=data.target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\n",
    "\n",
    "# 5. Train and evaluate SVM\n",
    "print(\"\\n=== Linear SVM (with class weights) ===\")\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=data.target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# 6. ROC-AUC for Naïve Bayes (since it provides probabilities)\n",
    "y_prob_nb = nb_pipeline.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_nb)\n",
    "print(\"\\nROC-AUC (Naïve Bayes):\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3281e-fde3-43d3-9e00-52f1e89092b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
